{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exercise: Full-fine tuning BERT\n",
        "\n",
        "In this exercise, you will create a BERT sentiment classifier (actually DistilBERT) using the [Hugging Face Transformers](https://huggingface.co/transformers/) library. You will use the [IMDB movie review dataset](https://ai.stanford.edu/~amaas/data/sentiment/) to complete a full fine-tuning and evaluate your model.\n",
        "\n",
        "The IMDB dataset contains movie reviews that are labeled as either positive or negative."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/briancruz/school/generative-ai-course-generator/.venv/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n",
            "/Users/briancruz/school/generative-ai-course-generator/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['sms', 'label'],\n",
              "    num_rows: 4459\n",
              "})"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load the sms_spam dataset\n",
        "# See: https://huggingface.co/datasets/sms_spam\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "# The sms_spam dataset only has a train split, so we use the train_test_split method to split it into train and test\n",
        "dataset = load_dataset(\"sms_spam\", split=\"train\").train_test_split(\n",
        "    test_size=0.2, shuffle=True, seed=23\n",
        ")\n",
        "\n",
        "splits = [\"train\", \"test\"]\n",
        "\n",
        "# View the dataset characteristics\n",
        "dataset[\"train\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's look at the first example!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'sms': 'Had your mobile 10 mths? Update to the latest Camera/Video phones for FREE. KEEP UR SAME NUMBER, Get extra free mins/texts. Text YES for a call\\n',\n",
              " 'label': 1}"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Inspect the first example. Do you think this is spam or not?\n",
        "dataset[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pre-process datasets\n",
        "\n",
        "Now we are going to process our datasets by converting all the text into tokens for our models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['sms', 'label', 'input_ids', 'attention_mask'],\n",
              "    num_rows: 4459\n",
              "})"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "# Let's use a lambda function to tokenize all the examples\n",
        "tokenized_dataset = {}\n",
        "for split in splits:\n",
        "    tokenized_dataset[split] = dataset[split].map(\n",
        "        lambda x: tokenizer(x[\"sms\"], truncation=True), batched=True\n",
        "    )\n",
        "\n",
        "# Inspect the available columns in the dataset\n",
        "tokenized_dataset[\"train\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load and set up the model\n",
        "\n",
        "In this case we are doing a full fine tuning, so we will want to unfreeze all parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Replace <MASK> with th code to unfreeze all the model parameters\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"distilbert-base-uncased\",\n",
        "    num_labels=2,\n",
        "    id2label={0: \"not spam\", 1: \"spam\"},\n",
        "    label2id={\"not spam\": 0, \"spam\": 1},\n",
        ")\n",
        "\n",
        "# Unfreeze all the model parameters.\n",
        "# Hint: Check the documentation at https://huggingface.co/transformers/v4.2.2/training.html\n",
        "for param in model.parameters():\n",
        "    <MASK>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Replace <MASK> with th code to unfreeze all the model parameters\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"distilbert-base-uncased\",\n",
        "    num_labels=2,\n",
        "    id2label={0: \"not spam\", 1: \"spam\"},\n",
        "    label2id={\"not spam\": 0, \"spam\": 1},\n",
        ")\n",
        "\n",
        "# Unfreeze all the model parameters.\n",
        "# Hint: Check the documentation at https://huggingface.co/transformers/v4.2.2/training.html\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DistilBertForSequenceClassification(\n",
            "  (distilbert): DistilBertModel(\n",
            "    (embeddings): Embeddings(\n",
            "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (transformer): Transformer(\n",
            "      (layer): ModuleList(\n",
            "        (0-5): 6 x TransformerBlock(\n",
            "          (attention): MultiHeadSelfAttention(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (ffn): FFN(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (activation): GELUActivation()\n",
            "          )\n",
            "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Let's train it!\n",
        "\n",
        "Now it's time to train our model. We'll use the `Trainer` class.\n",
        "\n",
        "First we'll define a function to compute our accuracy metreic then we make the `Trainer`.\n",
        "\n",
        "In this instance, we will fill in some of the training arguments\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "  0%|          | 0/558 [00:00<?, ?it/s]You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            " 50%|█████     | 279/558 [00:50<01:55,  2.42it/s]\n",
            " 50%|█████     | 279/558 [00:54<01:55,  2.42it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.04933308809995651, 'eval_accuracy': 0.989237668161435, 'eval_runtime': 4.2749, 'eval_samples_per_second': 260.822, 'eval_steps_per_second': 16.374, 'epoch': 1.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 90%|████████▉ | 501/558 [01:26<00:08,  6.40it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0541, 'learning_rate': 2.078853046594982e-06, 'epoch': 1.79}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 558/558 [01:34<00:00,  5.30it/s]\n",
            "100%|██████████| 558/558 [01:37<00:00,  5.30it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.050008438527584076, 'eval_accuracy': 0.9883408071748879, 'eval_runtime': 2.2136, 'eval_samples_per_second': 503.697, 'eval_steps_per_second': 31.622, 'epoch': 2.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 558/558 [01:38<00:00,  5.67it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train_runtime': 98.3909, 'train_samples_per_second': 90.638, 'train_steps_per_second': 5.671, 'train_loss': 0.052075445865644776, 'epoch': 2.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=558, training_loss=0.052075445865644776, metrics={'train_runtime': 98.3909, 'train_samples_per_second': 90.638, 'train_steps_per_second': 5.671, 'train_loss': 0.052075445865644776, 'epoch': 2.0})"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Replace <MASK> with the Training Arguments of your choice\n",
        "\n",
        "import numpy as np\n",
        "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return {\"accuracy\": (predictions == labels).mean()}\n",
        "\n",
        "\n",
        "# The HuggingFace Trainer class handles the training and eval loop for PyTorch for us.\n",
        "# Read more about it here https://huggingface.co/docs/transformers/main_classes/trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=TrainingArguments(\n",
        "        output_dir=\"./data/spam_not_spam\",\n",
        "        # Set the learning rate\n",
        "        <MASK>\n",
        "        # Set the per device train batch size and eval batch size\n",
        "        <MASK>\n",
        "        <MASK>\n",
        "        # Evaluate and save the model after each epoch\n",
        "        <MASK>\n",
        "        <MASK>\n",
        "        num_train_epochs=2,\n",
        "        weight_decay=0.01,\n",
        "        load_best_model_at_end=True,\n",
        "    ),\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate the model\n",
        "\n",
        "Evaluating the model is as simple as calling the evaluate method on the trainer object. This will run the model on the test set and compute the metrics we specified in the compute_metrics function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 70/70 [00:02<00:00, 31.96it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 0.03742317110300064,\n",
              " 'eval_accuracy': 0.9910313901345291,\n",
              " 'eval_runtime': 2.3006,\n",
              " 'eval_samples_per_second': 484.656,\n",
              " 'eval_steps_per_second': 30.427,\n",
              " 'epoch': 2.0}"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Show the performance of the model on the test set\n",
        "# What do you think the evaluation accuracy will be?\n",
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### View the results\n",
        "\n",
        "Let's look at a few examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 1149.44it/s]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sms</th>\n",
              "      <th>predictions</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Yup... Hey then one day on fri we can ask miwa and jiayin take leave go karaoke \\n</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Happy new years melody!\\n</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>PRIVATE! Your 2003 Account Statement for shows 800 un-redeemed S. I. M. points. Call 08715203652 Identifier Code: 42810 Expires 29/10/0\\n</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>URGENT! We are trying to contact U. Todays draw shows that you have won a £800 prize GUARANTEED. Call 09050003091 from land line. Claim C52. Valid 12hrs only\\n</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I had askd u a question some hours before. Its answer\\n</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>SMS. ac JSco: Energy is high, but u may not know where 2channel it. 2day ur leadership skills r strong. Psychic? Reply ANS w/question. End? Reply END JSCO\\n</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Yun ah.the ubi one say if ü wan call by tomorrow.call 67441233 look for irene.ere only got bus8,22,65,61,66,382. Ubi cres,ubi tech park.6ph for 1st 5wkg days.èn\\n</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Burger King - Wanna play footy at a top stadium? Get 2 Burger King before 1st Sept and go Large or Super with Coca-Cola and walk out a winner\\n</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                  sms  \\\n",
              "0                                                                                  Yup... Hey then one day on fri we can ask miwa and jiayin take leave go karaoke \\n   \n",
              "1                                                                                                                                           Happy new years melody!\\n   \n",
              "2                           PRIVATE! Your 2003 Account Statement for shows 800 un-redeemed S. I. M. points. Call 08715203652 Identifier Code: 42810 Expires 29/10/0\\n   \n",
              "3     URGENT! We are trying to contact U. Todays draw shows that you have won a £800 prize GUARANTEED. Call 09050003091 from land line. Claim C52. Valid 12hrs only\\n   \n",
              "4                                                                                                             I had askd u a question some hours before. Its answer\\n   \n",
              "5        SMS. ac JSco: Energy is high, but u may not know where 2channel it. 2day ur leadership skills r strong. Psychic? Reply ANS w/question. End? Reply END JSCO\\n   \n",
              "6  Yun ah.the ubi one say if ü wan call by tomorrow.call 67441233 look for irene.ere only got bus8,22,65,61,66,382. Ubi cres,ubi tech park.6ph for 1st 5wkg days.èn\\n   \n",
              "7                     Burger King - Wanna play footy at a top stadium? Get 2 Burger King before 1st Sept and go Large or Super with Coca-Cola and walk out a winner\\n   \n",
              "\n",
              "   predictions  labels  \n",
              "0            0       0  \n",
              "1            0       0  \n",
              "2            1       1  \n",
              "3            1       1  \n",
              "4            0       0  \n",
              "5            0       1  \n",
              "6            1       0  \n",
              "7            1       1  "
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Make a dataframe with the predictions and the text and the labels\n",
        "import pandas as pd\n",
        "\n",
        "items_for_manual_review = tokenized_dataset[\"test\"].select(\n",
        "    [0, 1, 22, 31, 43, 292, 448, 487]\n",
        ")\n",
        "\n",
        "results = trainer.predict(items_for_manual_review)\n",
        "df = pd.DataFrame(\n",
        "    {\n",
        "        \"sms\": [item[\"sms\"] for item in items_for_manual_review],\n",
        "        \"predictions\": results.predictions.argmax(axis=1),\n",
        "        \"labels\": results.label_ids,\n",
        "    }\n",
        ")\n",
        "# Show all the cell\n",
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### End of the exercise\n",
        "\n",
        "Great work! Congrats on making it to the end of the exercise!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
