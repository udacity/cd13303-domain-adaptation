{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Full-fine tuning BERT\n",
    "\n",
    "In this exercise, you will create a BERT sentiment classifier using the [Hugging Face Transformers](https://huggingface.co/transformers/) library. You will use the [IMDB movie review dataset](https://ai.stanford.edu/~amaas/data/sentiment/) to train and evaluate your model.\n",
    "\n",
    "The IMDB dataset contains 50,000 movie reviews that are labeled as either positive or negative. The dataset is split into 25,000 reviews for training and 25,000 reviews for testing. The training and testing sets are balanced, meaning they contain an equal number of positive and negative reviews.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies for this notebook if they are missing\n",
    "! pip install -q \\\n",
    "    scikit-learn \\\n",
    "    evaluate \\\n",
    "    datasets \\\n",
    "    \"transformers[torch]\" \\\n",
    "    ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Oct  9 03:26:12 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.113.01             Driver Version: 535.113.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1060 3GB    Off | 00000000:01:00.0 Off |                  N/A |\n",
      "| 12%   50C    P5               8W / 120W |      0MiB /  3072MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Since we intend on using a GPU, we inspect that one is available and has enough memory to run the model\n",
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sms': 'Had your mobile 10 mths? Update to the latest Camera/Video phones for FREE. KEEP UR SAME NUMBER, Get extra free mins/texts. Text YES for a call\\n',\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the datasets and transformers packages\n",
    "# NOTE: If you receieve an error such as \"ModuleNotFoundError: No module named 'datasets'\",\n",
    "# please restart the kernel (Kernel > Restart) and start the notebook from the top\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "splits = [\"train\", \"test\"]\n",
    "\n",
    "# The sms_spam dataset only has a train split, so we use the train_test_split method to split it into train and test\n",
    "dataset = load_dataset(\"sms_spam\", split=\"train\").train_test_split(test_size=0.2, shuffle=True, seed=23)\n",
    "\n",
    "\n",
    "dataset[\"train\"][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2018, 2115, 4684, 2184, 11047, 7898, 1029, 10651, 2000, 1996, 6745, 4950, 1013, 2678, 11640, 2005, 2489, 1012, 2562, 24471, 2168, 2193, 1010, 2131, 4469, 2489, 8117, 2015, 1013, 6981, 1012, 3793, 2748, 2005, 1037, 2655, 102]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "tokenized_dataset = {}\n",
    "for split in splits:\n",
    "    tokenized_dataset[split] = dataset[split].map(lambda x: tokenizer(x[\"sms\"], truncation=True), batched=True)\n",
    "print(tokenized_dataset[\"train\"][0][\"input_ids\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import evaluate\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "id2label = {0: \"not spam\", 1: \"spam\"}\n",
    "label2id = {\"not spam\": 0, \"spam\": 1}\n",
    "\n",
    "\n",
    "# https://huggingface.co/transformers/v3.0.2/model_doc/auto.html#automodelforsequenceclassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id\n",
    ")\n",
    "\n",
    "# Unfreeze all the model parameters.\n",
    "# Note: \n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "    # adjust learning rate for fine-tuning by uncommenting the following line\n",
    "    # param.requires_grad_(lr=2e-5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(model.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='558' max='558' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [558/558 01:44, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.050394</td>\n",
       "      <td>0.988341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.052400</td>\n",
       "      <td>0.047742</td>\n",
       "      <td>0.989238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=558, training_loss=0.049542664199747066, metrics={'train_runtime': 104.8336, 'train_samples_per_second': 85.068, 'train_steps_per_second': 5.323, 'total_flos': 143323774661868.0, 'train_loss': 0.049542664199747066, 'epoch': 2.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# The HuggingFace Trainer class handles the training and eval loop for PyTorch for us.\n",
    "# Read more about it here https://huggingface.co/docs/transformers/main_classes/trainer \n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./data/sentiment_analysis\",\n",
    "        learning_rate=2e-5,\n",
    "        # Reduce the batch size if you don't have enough memory\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=2,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        push_to_hub=False,\n",
    "\n",
    "    ),\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sms</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yup... Hey then one day on fri we can ask miwa and jiayin take leave go karaoke \\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Happy new years melody!\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Think I could stop by in like an hour or so? My roommate's looking to stock up for a trip\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I can make lasagna for you... vodka...\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No rushing. I'm not working. I'm in school so if we rush we go hungry.\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>PRIVATE! Your 2003 Account Statement for shows 800 un-redeemed S. I. M. points. Call 08715203652 Identifier Code: 42810 Expires 29/10/0\\n</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>URGENT! We are trying to contact U. Todays draw shows that you have won a £800 prize GUARANTEED. Call 09050003091 from land line. Claim C52. Valid 12hrs only\\n</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>I want some cock! My hubby's away, I need a real man 2 satisfy me. Txt WIFE to 89938 for no strings action. (Txt STOP 2 end, txt rec £1.50ea. OTBox 731 LA1 7WS. )\\n</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Your unique user ID is 1172. For removal send STOP to 87239 customer services 08708034412\\n</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Double your mins &amp; txts on Orange or 1/2 price linerental - Motorola and SonyEricsson with B/Tooth FREE-Nokia FREE Call MobileUpd8 on 08000839402 or2optout/HV9D\\n</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                     sms  \\\n",
       "0                                                                                     Yup... Hey then one day on fri we can ask miwa and jiayin take leave go karaoke \\n   \n",
       "1                                                                                                                                              Happy new years melody!\\n   \n",
       "2                                                                            Think I could stop by in like an hour or so? My roommate's looking to stock up for a trip\\n   \n",
       "3                                                                                                                               I can make lasagna for you... vodka...\\n   \n",
       "4                                                                                               No rushing. I'm not working. I'm in school so if we rush we go hungry.\\n   \n",
       "22                             PRIVATE! Your 2003 Account Statement for shows 800 un-redeemed S. I. M. points. Call 08715203652 Identifier Code: 42810 Expires 29/10/0\\n   \n",
       "31       URGENT! We are trying to contact U. Todays draw shows that you have won a £800 prize GUARANTEED. Call 09050003091 from land line. Claim C52. Valid 12hrs only\\n   \n",
       "48  I want some cock! My hubby's away, I need a real man 2 satisfy me. Txt WIFE to 89938 for no strings action. (Txt STOP 2 end, txt rec £1.50ea. OTBox 731 LA1 7WS. )\\n   \n",
       "49                                                                           Your unique user ID is 1172. For removal send STOP to 87239 customer services 08708034412\\n   \n",
       "54    Double your mins & txts on Orange or 1/2 price linerental - Motorola and SonyEricsson with B/Tooth FREE-Nokia FREE Call MobileUpd8 on 08000839402 or2optout/HV9D\\n   \n",
       "\n",
       "    label  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  \n",
       "22      1  \n",
       "31      1  \n",
       "48      1  \n",
       "49      1  \n",
       "54      1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(tokenized_dataset[\"test\"])\n",
    "df = df[['sms', 'label']]\n",
    "df = pd.concat(\n",
    "    [\n",
    "        df[df['label'] == 0].head(5),\n",
    "        df[df['label'] == 1].head(5)\n",
    "    ]\n",
    ")\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_indices = list(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. pred=not spam | label=not spam \n",
      " Yup... Hey then one day on fri we can ask miwa and jiayin take leave go karaoke \n",
      "\n",
      "1. pred=not spam | label=not spam \n",
      " Happy new years melody!\n",
      "\n",
      "2. pred=not spam | label=not spam \n",
      " Think I could stop by in like an hour or so? My roommate's looking to stock up for a trip\n",
      "\n",
      "3. pred=not spam | label=not spam \n",
      " I can make lasagna for you... vodka...\n",
      "\n",
      "4. pred=not spam | label=not spam \n",
      " No rushing. I'm not working. I'm in school so if we rush we go hungry.\n",
      "\n",
      "5. pred=spam | label=spam \n",
      " PRIVATE! Your 2003 Account Statement for shows 800 un-redeemed S. I. M. points. Call 08715203652 Identifier Code: 42810 Expires 29/10/0\n",
      "\n",
      "6. pred=spam | label=spam \n",
      " URGENT! We are trying to contact U. Todays draw shows that you have won a £800 prize GUARANTEED. Call 09050003091 from land line. Claim C52. Valid 12hrs only\n",
      "\n",
      "7. pred=spam | label=spam \n",
      " I want some cock! My hubby's away, I need a real man 2 satisfy me. Txt WIFE to 89938 for no strings action. (Txt STOP 2 end, txt rec £1.50ea. OTBox 731 LA1\n",
      " 7WS. )\n",
      "\n",
      "8. pred=spam | label=spam \n",
      " Your unique user ID is 1172. For removal send STOP to 87239 customer services 08708034412\n",
      "\n",
      "9. pred=spam | label=spam \n",
      " Double your mins & txts on Orange or 1/2 price linerental - Motorola and SonyEricsson with B/Tooth FREE-Nokia FREE Call MobileUpd8 on 08000839402\n",
      " or2optout/HV9D\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the performance of the model on some test set examples\n",
    "\n",
    "# select the first negative and the first positive test set examples\n",
    "\n",
    "\n",
    "predictions = trainer.predict(tokenized_dataset[\"test\"].select(dataset_indices))\n",
    "# Print the 10 first test set samples with their predictions.\n",
    "# split text into lines of 80 characters\n",
    "def split_text(text, n=160):\n",
    "    lines = []\n",
    "    while len(text) > n:\n",
    "        line = text[:n]\n",
    "        space_index = line.rfind(\" \")\n",
    "        if space_index != -1:\n",
    "            line = line[:space_index]\n",
    "        lines.append(line)\n",
    "        text = text[len(line):]\n",
    "    lines.append(text)\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "for index, (pred, label, text) in enumerate(zip(predictions.predictions.argmax(axis=1), predictions.label_ids, tokenized_dataset[\"test\"].select(dataset_indices)[\"sms\"])):\n",
    "    print(f\"{index}. pred={id2label[pred]} | label={id2label[label]} \\n {split_text(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([-0.0166, -0.0666, -0.0163, -0.0421, -0.0080, -0.0140, -0.0635, -0.0205,\n",
       "         -0.0086, -0.0634], device='cuda:0'),\n",
       " tensor([-0.0026,  0.0226, -0.0206,  0.0321, -0.0106,  0.0477,  0.0287,  0.0287,\n",
       "          0.0303,  0.0136], device='cuda:0'),\n",
       " tensor([ 0.0913, -0.0141, -0.0760,  0.1141, -0.0585,  0.0501, -0.1948,  0.0975,\n",
       "          0.0030,  0.0678], device='cuda:0'),\n",
       " tensor([ 0.0705,  0.0559,  0.0193, -0.1212, -0.0017, -0.0213, -0.0078, -0.0307,\n",
       "          0.0206, -0.0392], device='cuda:0'),\n",
       " tensor([ 0.0141, -0.0824,  0.0225, -0.0240, -0.0157, -0.0536, -0.0005,  0.0093,\n",
       "          0.0343,  0.0108], device='cuda:0'),\n",
       " tensor([ 0.0511,  0.0027, -0.0665, -0.0275,  0.0445, -0.0402,  0.0479,  0.0373,\n",
       "          0.0264, -0.0376], device='cuda:0'),\n",
       " tensor([-0.0548, -0.0462, -0.0248, -0.0028,  0.0238,  0.0012,  0.0135, -0.0256,\n",
       "          0.0121,  0.0457], device='cuda:0'),\n",
       " tensor([-0.0122,  0.0332, -0.0152, -0.0170, -0.0125,  0.0020,  0.0004, -0.0015,\n",
       "          0.0328,  0.0198], device='cuda:0'),\n",
       " tensor([-0.0229, -0.0108, -0.0002,  0.0130,  0.0270,  0.0082, -0.0222,  0.0203,\n",
       "         -0.0250,  0.0002], device='cuda:0')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for param in model.parameters():\n",
    "#     # print the weights of the model\n",
    "#     print(param.data[0,:10])\n",
    "\n",
    "\n",
    "[model.base_model.embeddings.word_embeddings.weight.data[0,:10],\n",
    "model.base_model.transformer.layer[0].attention.q_lin.weight.data[0,:10],\n",
    "model.base_model.transformer.layer[1].attention.q_lin.weight.data[0,:10],\n",
    "model.base_model.transformer.layer[2].attention.q_lin.weight.data[0,:10],\n",
    "model.base_model.transformer.layer[3].attention.q_lin.weight.data[0,:10],\n",
    "model.base_model.transformer.layer[4].attention.q_lin.weight.data[0,:10],\n",
    "model.base_model.transformer.layer[5].attention.q_lin.weight.data[0,:10],\n",
    "model.pre_classifier.weight.data[0,:10],\n",
    "model.classifier.weight.data[0,:10],\n",
    "]\n",
    "\n",
    "# Verify that the weights of the model have changed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
