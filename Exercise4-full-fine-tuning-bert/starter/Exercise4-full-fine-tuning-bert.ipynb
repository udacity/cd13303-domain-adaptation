{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exercise: Full-fine tuning BERT\n",
        "\n",
        "In this exercise, you will create a BERT sentiment classifier using the [Hugging Face Transformers](https://huggingface.co/transformers/) library. You will use the [IMDB movie review dataset](https://ai.stanford.edu/~amaas/data/sentiment/) to train and evaluate your model.\n",
        "\n",
        "The IMDB dataset contains 50,000 movie reviews that are labeled as either positive or negative. The dataset is split into 25,000 reviews for training and 25,000 reviews for testing. The training and testing sets are balanced, meaning they contain an equal number of positive and negative reviews.\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies for this notebook if they are missing\n",
        "! pip install -q \\\n",
        "    scikit-learn \\\n",
        "    evaluate \\\n",
        "    datasets \\\n",
        "    \"transformers[torch]\" \\\n",
        "    ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Since we intend on using a GPU, we inspect that one is available and has enough memory to run the model\n",
        "! nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the sms_spam dataset\n",
        "# See: https://huggingface.co/datasets/sms_spam\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "# The sms_spam dataset only has a train split, so we use the train_test_split method to split it into train and test\n",
        "dataset = load_dataset(\"sms_spam\", split=\"train\").train_test_split(\n",
        "    test_size=0.2, shuffle=True, seed=23\n",
        ")\n",
        "\n",
        "splits = [\"train\", \"test\"]\n",
        "\n",
        "# View the dataset characteristics\n",
        "dataset[\"train\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspect the first example\n",
        "dataset[\"train\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inpsect another example\n",
        "print(dataset[\"train\"][2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True)\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "tokenized_dataset = {}\n",
        "for split in splits:\n",
        "    tokenized_dataset[split] = dataset[split].map(\n",
        "        lambda x: tokenizer(x[\"sms\"], truncation=True), batched=True\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenized_dataset[\"train\"]\n",
        "# print(tokenized_dataset[\"train\"][0][\"input_ids\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return accuracy.compute(predictions=predictions, references=labels)\n",
        "\n",
        "\n",
        "id2label = {0: \"not spam\", 1: \"spam\"}\n",
        "label2id = {\"not spam\": 0, \"spam\": 1}\n",
        "\n",
        "\n",
        "# https://huggingface.co/transformers/v3.0.2/model_doc/auto.html#automodelforsequenceclassification\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"distilbert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id\n",
        ")\n",
        "\n",
        "# Unfreeze all the model parameters.\n",
        "# Note:\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = True\n",
        "    # adjust learning rate for fine-tuning by uncommenting the following line\n",
        "    # param.requires_grad_(lr=2e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# list(model.named_parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "# The HuggingFace Trainer class handles the training and eval loop for PyTorch for us.\n",
        "# Read more about it here https://huggingface.co/docs/transformers/main_classes/trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=TrainingArguments(\n",
        "        output_dir=\"./data/sentiment_analysis\",\n",
        "        learning_rate=2e-5,\n",
        "        # Reduce the batch size if you don't have enough memory\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=16,\n",
        "        num_train_epochs=2,\n",
        "        weight_decay=0.01,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        push_to_hub=False,\n",
        "    ),\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# dataset_indices = list(df.index)\n",
        "items_for_manual_review = tokenized_dataset[\"test\"].select([0, 1, 22, 31])\n",
        "\n",
        "# show the dataset\n",
        "for item in items_for_manual_review:\n",
        "    print(item[\"sms\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = trainer.predict(items_for_manual_review)\n",
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results.predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results.predictions.argmax(axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make a dataframe with the predictions and the text and the labels\n",
        "\n",
        "df = pd.DataFrame(\n",
        "    {\n",
        "        \"sms\": [item[\"sms\"] for item in items_for_manual_review],\n",
        "        \"predictions\": results.predictions.argmax(axis=1),\n",
        "        \"labels\": results.label_ids,\n",
        "    }\n",
        ")\n",
        "df"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
