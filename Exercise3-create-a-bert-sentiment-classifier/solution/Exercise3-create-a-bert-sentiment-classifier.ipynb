{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exercise: Create a BERT sentiment classifier\n",
        "\n",
        "In this exercise, you will create a BERT sentiment classifier using the [Hugging Face Transformers](https://huggingface.co/transformers/) library. You will use the [IMDB movie review dataset](https://ai.stanford.edu/~amaas/data/sentiment/) to train and evaluate your model.\n",
        "\n",
        "The IMDB dataset contains 50,000 movie reviews that are labeled as either positive or negative. The dataset is split into 25,000 reviews for training and 25,000 reviews for testing. The training and testing sets are balanced, meaning they contain an equal number of positive and negative reviews.\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies for this notebook if they are missing\n",
        "! pip install -q \\\n",
        "    scikit-learn \\\n",
        "    evaluate \\\n",
        "    datasets \\\n",
        "    \"transformers[torch]\" \\\n",
        "    ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Since we intend on using a GPU, we inspect that one is available and has enough memory to run the model\n",
        "! nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import the datasets and transformers packages\n",
        "# NOTE: If you receieve an error such as \"ModuleNotFoundError: No module named 'datasets'\",\n",
        "# please restart the kernel (Kernel > Restart) and start the notebook from the top\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load the imdb dataset: https://huggingface.co/datasets/imdb\n",
        "# Take only the \"train\" and \"test\" splits\n",
        "splits = [\"train\", \"test\"]\n",
        "ds = {split: ds for split, ds in zip(splits,load_dataset(\"imdb\", split=splits))}\n",
        "\n",
        "# Thin out the dataset to make it run faster for this example\n",
        "for split in splits:\n",
        "    ds[split] = ds[split].shuffle(seed=42).select(range(100))\n",
        "\n",
        "# Show the first training example\n",
        "# dataset[\"train\"][0]\n",
        "# dataset[\"train\"]\n",
        "ds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "tokenized_ds = {}\n",
        "for split in splits:\n",
        "    tokenized_ds[split] = ds[split].map(lambda x: tokenizer(x[\"text\"], truncation=True), batched=True)\n",
        "print(tokenized_ds[\"train\"][0][\"input_ids\"])\n",
        "\n",
        "# batch_size = 32\n",
        "\n",
        "# train_loader = DataLoader(train_dataset.with_format(type='torch'), batch_size=32)\n",
        "\n",
        "# for batch_idx, (data, label) in enumerate(train_loader):\n",
        "#     print(batch_idx, data, label)\n",
        "#     if batch_idx > 10:\n",
        "#         break\n",
        "\n",
        "# train_dataset.with_format(\"torch\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import evaluate\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return accuracy.compute(predictions=predictions, references=labels)\n",
        "\n",
        "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
        "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
        "\n",
        "\n",
        "# https://huggingface.co/transformers/v3.0.2/model_doc/auto.html#automodelforsequenceclassification\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"distilbert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id\n",
        ")\n",
        "\n",
        "# Freeze all the parameters of the base model\n",
        "for param in model.base_model.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check which layers are frozen\n",
        "# for param in model.base_model.parameters():\n",
        "#     print(param.requires_grad)\n",
        "# len(list(model.base_model.parameters()))\n",
        "# print the nubmer of layers\n",
        "\n",
        "# Show the weights of the first 5 layers\n",
        "\n",
        "old = [model.base_model.embeddings.word_embeddings.weight.data[0,:10],\n",
        "model.base_model.transformer.layer[0].attention.q_lin.weight.data[0,:10],\n",
        "model.base_model.transformer.layer[1].attention.q_lin.weight.data[0,:10],\n",
        "model.base_model.transformer.layer[2].attention.q_lin.weight.data[0,:10],\n",
        "model.base_model.transformer.layer[3].attention.q_lin.weight.data[0,:10],\n",
        "model.base_model.transformer.layer[4].attention.q_lin.weight.data[0,:10],\n",
        "model.base_model.transformer.layer[5].attention.q_lin.weight.data[0,:10],\n",
        "model.pre_classifier.weight.data[0,:10],\n",
        "model.classifier.weight.data[0,:10],\n",
        "]\n",
        "print(old)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "# The HuggingFace Trainer class handles the training and eval loop for PyTorch for us.\n",
        "# Read more about it here https://huggingface.co/docs/transformers/main_classes/trainer \n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=TrainingArguments(\n",
        "        output_dir=\"./data/sentiment_analysis\",\n",
        "        learning_rate=2e-3,\n",
        "        # Reduce the batch size if you don't have enough memory\n",
        "        per_device_train_batch_size=4,\n",
        "        per_device_eval_batch_size=4,\n",
        "        num_train_epochs=1,\n",
        "        weight_decay=0.01,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        push_to_hub=False,\n",
        "\n",
        "    ),\n",
        "    train_dataset=tokenized_ds[\"train\"],\n",
        "    eval_dataset=tokenized_ds[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "new = [model.base_model.embeddings.word_embeddings.weight.data[0,:10],\n",
        "model.base_model.transformer.layer[0].attention.q_lin.weight.data[0,:10],\n",
        "model.base_model.transformer.layer[1].attention.q_lin.weight.data[0,:10],\n",
        "model.base_model.transformer.layer[2].attention.q_lin.weight.data[0,:10],\n",
        "model.base_model.transformer.layer[3].attention.q_lin.weight.data[0,:10],\n",
        "model.base_model.transformer.layer[4].attention.q_lin.weight.data[0,:10],\n",
        "model.base_model.transformer.layer[5].attention.q_lin.weight.data[0,:10],\n",
        "model.pre_classifier.weight.data[0,:10],\n",
        "model.classifier.weight.data[0,:10],\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(old)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(new)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "for old_item, new_item in zip(old, new):\n",
        "    # move to cpu device\n",
        "    old_item = old_item.cpu()\n",
        "    new_item = new_item.cpu()\n",
        "\n",
        "    print((torch.eq(old_item, new_item)))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(model.base_model.embeddings.word_embeddings.weight.data[0,:10])\n",
        "print(old_values)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show the performance of the model on the test set\n",
        "# What do you think the evaluation accuracy will be?\n",
        "trainer.evaluate()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(tokenized_ds[\"test\"])\n",
        "df = df[['text', 'label']]\n",
        "df = pd.concat(\n",
        "    [\n",
        "        df[df['label'] == 0].head(2),\n",
        "        df[df['label'] == 1].head(2)\n",
        "    ]\n",
        ")\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# show entire cell in pandas\n",
        "pd.set_option('display.max_colwidth', 200)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_indices = list(df.index)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show the performance of the model on some test set examples\n",
        "\n",
        "# select the first negative and the first positive test set examples\n",
        "\n",
        "\n",
        "predictions = trainer.predict(tokenized_ds[\"test\"].select(dataset_indices))\n",
        "# Print the 10 first test set samples with their predictions.\n",
        "# split text into lines of 80 characters\n",
        "def split_text(text, n=160):\n",
        "    lines = []\n",
        "    while len(text) > n:\n",
        "        line = text[:n]\n",
        "        space_index = line.rfind(\" \")\n",
        "        if space_index != -1:\n",
        "            line = line[:space_index]\n",
        "        lines.append(line)\n",
        "        text = text[len(line):]\n",
        "    lines.append(text)\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "for index, (pred, label, text) in enumerate(zip(predictions.predictions.argmax(axis=1), predictions.label_ids, tokenized_ds[\"test\"].select(dataset_indices)[\"text\"])):\n",
        "    print(f\"{index}. pred={id2label[pred]} | label={id2label[label]} \\n {split_text(text)}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
