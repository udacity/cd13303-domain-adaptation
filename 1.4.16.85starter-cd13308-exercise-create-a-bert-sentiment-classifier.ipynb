{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Create a BERT sentiment classifier\n",
    "\n",
    "In this exercise, you will create a BERT sentiment classifier using the [Hugging Face Transformers](https://huggingface.co/transformers/) library. You will use the [IMDB movie review dataset](https://ai.stanford.edu/~amaas/data/sentiment/) to train and evaluate your model.\n",
    "\n",
    "The IMDB dataset contains 50,000 movie reviews that are labeled as either positive or negative. The dataset is split into 25,000 reviews for training and 25,000 reviews for testing. The training and testing sets are balanced, meaning they contain an equal number of positive and negative reviews.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies for this notebook if they are missing\n",
    "! pip install -q \\\n",
    "    scikit-learn \\\n",
    "    evaluate \\\n",
    "    datasets \\\n",
    "    \"transformers[torch]\" \\\n",
    "    ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Oct  9 02:44:22 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.113.01             Driver Version: 535.113.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1060 3GB    Off | 00000000:01:00.0 Off |                  N/A |\n",
      "|  5%   50C    P5               8W / 120W |      0MiB /  3072MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Since we intend on using a GPU, we inspect that one is available and has enough memory to run the model\n",
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['text', 'label'],\n",
       "     num_rows: 100\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['text', 'label'],\n",
       "     num_rows: 100\n",
       " })}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the datasets and transformers packages\n",
    "# NOTE: If you receieve an error such as \"ModuleNotFoundError: No module named 'datasets'\",\n",
    "# please restart the kernel (Kernel > Restart) and start the notebook from the top\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the imdb dataset: https://huggingface.co/datasets/imdb\n",
    "# Take only the \"train\" and \"test\" splits\n",
    "splits = [\"train\", \"test\"]\n",
    "ds = {split: ds for split, ds in zip(splits,load_dataset(\"imdb\", split=splits))}\n",
    "\n",
    "# Thin out the dataset to make it run faster for this example\n",
    "for split in splits:\n",
    "    ds[split] = ds[split].shuffle(seed=42).select(range(100))\n",
    "\n",
    "# Show the first training example\n",
    "# dataset[\"train\"][0]\n",
    "# dataset[\"train\"]\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2045, 2003, 2053, 7189, 2012, 2035, 2090, 3481, 3771, 1998, 6337, 2099, 2021, 1996, 2755, 2008, 2119, 2024, 2610, 2186, 2055, 6355, 6997, 1012, 6337, 2099, 3504, 15594, 2100, 1010, 3481, 3771, 3504, 4438, 1012, 6337, 2099, 14811, 2024, 3243, 3722, 1012, 3481, 3771, 1005, 1055, 5436, 2024, 2521, 2062, 8552, 1012, 1012, 1012, 3481, 3771, 3504, 2062, 2066, 3539, 8343, 1010, 2065, 2057, 2031, 2000, 3962, 12319, 1012, 1012, 1012, 1996, 2364, 2839, 2003, 5410, 1998, 6881, 2080, 1010, 2021, 2031, 1000, 17936, 6767, 7054, 3401, 1000, 1012, 2111, 2066, 2000, 12826, 1010, 2000, 3648, 1010, 2000, 16157, 1012, 2129, 2055, 2074, 9107, 1029, 6057, 2518, 2205, 1010, 2111, 3015, 3481, 3771, 3504, 2137, 2021, 1010, 2006, 1996, 2060, 2192, 1010, 9177, 2027, 9544, 2137, 2186, 1006, 999, 999, 999, 1007, 1012, 2672, 2009, 1005, 1055, 1996, 2653, 1010, 2030, 1996, 4382, 1010, 2021, 1045, 2228, 2023, 2186, 2003, 2062, 2394, 2084, 2137, 1012, 2011, 1996, 2126, 1010, 1996, 5889, 2024, 2428, 2204, 1998, 6057, 1012, 1996, 3772, 2003, 2025, 23105, 2012, 2035, 1012, 1012, 1012, 102]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "tokenized_ds = {}\n",
    "for split in splits:\n",
    "    tokenized_ds[split] = ds[split].map(lambda x: tokenizer(x[\"text\"], truncation=True), batched=True)\n",
    "print(tokenized_ds[\"train\"][0][\"input_ids\"])\n",
    "\n",
    "# batch_size = 32\n",
    "\n",
    "# train_loader = DataLoader(train_dataset.with_format(type='torch'), batch_size=32)\n",
    "\n",
    "# for batch_idx, (data, label) in enumerate(train_loader):\n",
    "#     print(batch_idx, data, label)\n",
    "#     if batch_idx > 10:\n",
    "#         break\n",
    "\n",
    "# train_dataset.with_format(\"torch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import evaluate\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
    "\n",
    "\n",
    "# https://huggingface.co/transformers/v3.0.2/model_doc/auto.html#automodelforsequenceclassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id\n",
    ")\n",
    "\n",
    "# Freeze all the parameters of the base model\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([-0.0166, -0.0666, -0.0163, -0.0421, -0.0080, -0.0140, -0.0635, -0.0205,\n",
      "        -0.0086, -0.0634]), tensor([-0.0024,  0.0224, -0.0207,  0.0318, -0.0102,  0.0476,  0.0284,  0.0286,\n",
      "         0.0307,  0.0135]), tensor([ 0.0915, -0.0140, -0.0762,  0.1138, -0.0585,  0.0498, -0.1951,  0.0972,\n",
      "         0.0030,  0.0676]), tensor([ 0.0704,  0.0561,  0.0193, -0.1206, -0.0019, -0.0209, -0.0077, -0.0309,\n",
      "         0.0206, -0.0391]), tensor([ 0.0137, -0.0824,  0.0224, -0.0237, -0.0156, -0.0535, -0.0006,  0.0089,\n",
      "         0.0343,  0.0110]), tensor([ 0.0507,  0.0030, -0.0672, -0.0274,  0.0443, -0.0399,  0.0480,  0.0366,\n",
      "         0.0262, -0.0375]), tensor([-0.0548, -0.0465, -0.0241, -0.0025,  0.0238,  0.0011,  0.0133, -0.0253,\n",
      "         0.0117,  0.0457]), tensor([-0.0105, -0.0076, -0.0418,  0.0027, -0.0123, -0.0361,  0.0384,  0.0258,\n",
      "         0.0026, -0.0067]), tensor([-0.0377, -0.0157,  0.0238,  0.0043, -0.0064, -0.0117,  0.0127,  0.0111,\n",
      "        -0.0002, -0.0161])]\n"
     ]
    }
   ],
   "source": [
    "# Check which layers are frozen\n",
    "# for param in model.base_model.parameters():\n",
    "#     print(param.requires_grad)\n",
    "# len(list(model.base_model.parameters()))\n",
    "# print the nubmer of layers\n",
    "\n",
    "# Show the weights of the first 5 layers\n",
    "\n",
    "old = [model.base_model.embeddings.word_embeddings.weight.data[0,:10],\n",
    "model.base_model.transformer.layer[0].attention.q_lin.weight.data[0,:10],\n",
    "model.base_model.transformer.layer[1].attention.q_lin.weight.data[0,:10],\n",
    "model.base_model.transformer.layer[2].attention.q_lin.weight.data[0,:10],\n",
    "model.base_model.transformer.layer[3].attention.q_lin.weight.data[0,:10],\n",
    "model.base_model.transformer.layer[4].attention.q_lin.weight.data[0,:10],\n",
    "model.base_model.transformer.layer[5].attention.q_lin.weight.data[0,:10],\n",
    "model.pre_classifier.weight.data[0,:10],\n",
    "model.classifier.weight.data[0,:10],\n",
    "]\n",
    "print(old)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:06, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.650257</td>\n",
       "      <td>0.610000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# The HuggingFace Trainer class handles the training and eval loop for PyTorch for us.\n",
    "# Read more about it here https://huggingface.co/docs/transformers/main_classes/trainer \n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./data/sentiment_analysis\",\n",
    "        learning_rate=2e-3,\n",
    "        # Reduce the batch size if you don't have enough memory\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        num_train_epochs=1,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        push_to_hub=False,\n",
    "\n",
    "    ),\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "new = [model.base_model.embeddings.word_embeddings.weight.data[0,:10],\n",
    "model.base_model.transformer.layer[0].attention.q_lin.weight.data[0,:10],\n",
    "model.base_model.transformer.layer[1].attention.q_lin.weight.data[0,:10],\n",
    "model.base_model.transformer.layer[2].attention.q_lin.weight.data[0,:10],\n",
    "model.base_model.transformer.layer[3].attention.q_lin.weight.data[0,:10],\n",
    "model.base_model.transformer.layer[4].attention.q_lin.weight.data[0,:10],\n",
    "model.base_model.transformer.layer[5].attention.q_lin.weight.data[0,:10],\n",
    "model.pre_classifier.weight.data[0,:10],\n",
    "model.classifier.weight.data[0,:10],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([-0.0166, -0.0666, -0.0163, -0.0421, -0.0080, -0.0140, -0.0635, -0.0205,\n",
      "        -0.0086, -0.0634]), tensor([-0.0024,  0.0224, -0.0207,  0.0318, -0.0102,  0.0476,  0.0284,  0.0286,\n",
      "         0.0307,  0.0135]), tensor([ 0.0915, -0.0140, -0.0762,  0.1138, -0.0585,  0.0498, -0.1951,  0.0972,\n",
      "         0.0030,  0.0676]), tensor([ 0.0704,  0.0561,  0.0193, -0.1206, -0.0019, -0.0209, -0.0077, -0.0309,\n",
      "         0.0206, -0.0391]), tensor([ 0.0137, -0.0824,  0.0224, -0.0237, -0.0156, -0.0535, -0.0006,  0.0089,\n",
      "         0.0343,  0.0110]), tensor([ 0.0507,  0.0030, -0.0672, -0.0274,  0.0443, -0.0399,  0.0480,  0.0366,\n",
      "         0.0262, -0.0375]), tensor([-0.0548, -0.0465, -0.0241, -0.0025,  0.0238,  0.0011,  0.0133, -0.0253,\n",
      "         0.0117,  0.0457]), tensor([-0.0105, -0.0076, -0.0418,  0.0027, -0.0123, -0.0361,  0.0384,  0.0258,\n",
      "         0.0026, -0.0067]), tensor([-0.0377, -0.0157,  0.0238,  0.0043, -0.0064, -0.0117,  0.0127,  0.0111,\n",
      "        -0.0002, -0.0161])]\n"
     ]
    }
   ],
   "source": [
    "print(old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([-0.0166, -0.0666, -0.0163, -0.0421, -0.0080, -0.0140, -0.0635, -0.0205,\n",
      "        -0.0086, -0.0634], device='cuda:0'), tensor([-0.0024,  0.0224, -0.0207,  0.0318, -0.0102,  0.0476,  0.0284,  0.0286,\n",
      "         0.0307,  0.0135], device='cuda:0'), tensor([ 0.0915, -0.0140, -0.0762,  0.1138, -0.0585,  0.0498, -0.1951,  0.0972,\n",
      "         0.0030,  0.0676], device='cuda:0'), tensor([ 0.0704,  0.0561,  0.0193, -0.1206, -0.0019, -0.0209, -0.0077, -0.0309,\n",
      "         0.0206, -0.0391], device='cuda:0'), tensor([ 0.0137, -0.0824,  0.0224, -0.0237, -0.0156, -0.0535, -0.0006,  0.0089,\n",
      "         0.0343,  0.0110], device='cuda:0'), tensor([ 0.0507,  0.0030, -0.0672, -0.0274,  0.0443, -0.0399,  0.0480,  0.0366,\n",
      "         0.0262, -0.0375], device='cuda:0'), tensor([-0.0548, -0.0465, -0.0241, -0.0025,  0.0238,  0.0011,  0.0133, -0.0253,\n",
      "         0.0117,  0.0457], device='cuda:0'), tensor([-0.0144, -0.0062, -0.0414,  0.0037, -0.0083, -0.0350,  0.0393,  0.0283,\n",
      "         0.0024, -0.0148], device='cuda:0'), tensor([-0.0338, -0.0150,  0.0167,  0.0088,  0.0030, -0.0117,  0.0180,  0.0160,\n",
      "         0.0051, -0.0086], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "print(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True, True, True, True, True, True])\n",
      "tensor([True, True, True, True, True, True, True, True, True, True])\n",
      "tensor([True, True, True, True, True, True, True, True, True, True])\n",
      "tensor([True, True, True, True, True, True, True, True, True, True])\n",
      "tensor([True, True, True, True, True, True, True, True, True, True])\n",
      "tensor([True, True, True, True, True, True, True, True, True, True])\n",
      "tensor([True, True, True, True, True, True, True, True, True, True])\n",
      "tensor([False, False, False, False, False, False, False, False, False, False])\n",
      "tensor([False, False, False, False, False, False, False, False, False, False])\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "for old_item, new_item in zip(old, new):\n",
    "    # move to cpu device\n",
    "    old_item = old_item.cpu()\n",
    "    new_item = new_item.cpu()\n",
    "\n",
    "    print((torch.eq(old_item, new_item)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0166, -0.0666, -0.0163, -0.0421, -0.0080, -0.0140, -0.0635, -0.0205,\n",
      "        -0.0086, -0.0634], device='cuda:0')\n",
      "tensor([-0.0166, -0.0666, -0.0163, -0.0421, -0.0080, -0.0140, -0.0635, -0.0205,\n",
      "        -0.0086, -0.0634])\n"
     ]
    }
   ],
   "source": [
    "print(model.base_model.embeddings.word_embeddings.weight.data[0,:10])\n",
    "print(old_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.35444939136505127,\n",
       " 'eval_accuracy': 0.89,\n",
       " 'eval_runtime': 2.1459,\n",
       " 'eval_samples_per_second': 46.6,\n",
       " 'eval_steps_per_second': 11.65,\n",
       " 'epoch': 8.0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the performance of the model on the test set\n",
    "# What do you think the evaluation accuracy will be?\n",
    "trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This movie was so frustrating. Everything seemed energetic and I was totally prepared to have a good time. I at least thought I'd be able to stand it. But, I was wrong. First, the weird looping? I...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This movie spends most of its time preaching that it is the script that makes the movie, but apparently there was no script when they shot this waste of time! The trailer makes this out to be a co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;br /&gt;&lt;br /&gt;When I unsuspectedly rented A Thousand Acres, I thought I was in for an entertaining King Lear story and of course Michelle Pfeiffer was in it, so what could go wrong?&lt;br /&gt;&lt;br /&gt;Very ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is the latest entry in the long series of films with the French agent, O.S.S. 117 (the French answer to James Bond). The series was launched in the early 1950's, and spawned at least eight fi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                      text  \\\n",
       "2  This movie was so frustrating. Everything seemed energetic and I was totally prepared to have a good time. I at least thought I'd be able to stand it. But, I was wrong. First, the weird looping? I...   \n",
       "4  This movie spends most of its time preaching that it is the script that makes the movie, but apparently there was no script when they shot this waste of time! The trailer makes this out to be a co...   \n",
       "0  <br /><br />When I unsuspectedly rented A Thousand Acres, I thought I was in for an entertaining King Lear story and of course Michelle Pfeiffer was in it, so what could go wrong?<br /><br />Very ...   \n",
       "1  This is the latest entry in the long series of films with the French agent, O.S.S. 117 (the French answer to James Bond). The series was launched in the early 1950's, and spawned at least eight fi...   \n",
       "\n",
       "   label  \n",
       "2      0  \n",
       "4      0  \n",
       "0      1  \n",
       "1      1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(tokenized_ds[\"test\"])\n",
    "df = df[['text', 'label']]\n",
    "df = pd.concat(\n",
    "    [\n",
    "        df[df['label'] == 0].head(2),\n",
    "        df[df['label'] == 1].head(2)\n",
    "    ]\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This movie was so frustrating. Everything seemed energetic and I was totally prepared to have a good time. I at least thought I'd be able to stand it. But, I was wrong. First, the weird looping? I...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This movie spends most of its time preaching that it is the script that makes the movie, but apparently there was no script when they shot this waste of time! The trailer makes this out to be a co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;br /&gt;&lt;br /&gt;When I unsuspectedly rented A Thousand Acres, I thought I was in for an entertaining King Lear story and of course Michelle Pfeiffer was in it, so what could go wrong?&lt;br /&gt;&lt;br /&gt;Very ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is the latest entry in the long series of films with the French agent, O.S.S. 117 (the French answer to James Bond). The series was launched in the early 1950's, and spawned at least eight fi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                      text  \\\n",
       "2  This movie was so frustrating. Everything seemed energetic and I was totally prepared to have a good time. I at least thought I'd be able to stand it. But, I was wrong. First, the weird looping? I...   \n",
       "4  This movie spends most of its time preaching that it is the script that makes the movie, but apparently there was no script when they shot this waste of time! The trailer makes this out to be a co...   \n",
       "0  <br /><br />When I unsuspectedly rented A Thousand Acres, I thought I was in for an entertaining King Lear story and of course Michelle Pfeiffer was in it, so what could go wrong?<br /><br />Very ...   \n",
       "1  This is the latest entry in the long series of films with the French agent, O.S.S. 117 (the French answer to James Bond). The series was launched in the early 1950's, and spawned at least eight fi...   \n",
       "\n",
       "   label  \n",
       "2      0  \n",
       "4      0  \n",
       "0      1  \n",
       "1      1  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show entire cell in pandas\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_indices = list(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. pred=NEGATIVE | label=NEGATIVE \n",
      " This movie was so frustrating. Everything seemed energetic and I was totally prepared to have a good time. I at least thought I'd be able to stand it. But, I\n",
      " was wrong. First, the weird looping? It was like watching \"America's Funniest Home Videos\". The damn parents. I hated them so much. The stereo-typical Latino\n",
      " family? I need to speak with the person responsible for this. We need to have a talk. That little girl who was always hanging on someone? I just hated her and\n",
      " had to mention it. Now, the final scene transcends, I must say. It's so gloriously bad and full of badness that it is a movie of its own. What crappy dancing.\n",
      " Horrible and beautiful at once.\n",
      "1. pred=NEGATIVE | label=NEGATIVE \n",
      " This movie spends most of its time preaching that it is the script that makes the movie, but apparently there was no script when they shot this waste of time!\n",
      " The trailer makes this out to be a comedy, but the film can't decide if it wants to be a comedy, a drama, a romance or an action film. Press releases\n",
      " indicated that Shatner and Hamlin made this movie because they loved the script (what were they thinking?). If you like William Shatner (I do) see \"Free\n",
      " Enterprise\" instead.\n",
      "2. pred=POSITIVE | label=POSITIVE \n",
      " <br /><br />When I unsuspectedly rented A Thousand Acres, I thought I was in for an entertaining King Lear story and of course Michelle Pfeiffer was in it, so\n",
      " what could go wrong?<br /><br />Very quickly, however, I realized that this story was about A Thousand Other Things besides just Acres. I started crying and\n",
      " couldn't stop until long after the movie ended. Thank you Jane, Laura and Jocelyn, for bringing us such a wonderfully subtle and compassionate movie! Thank\n",
      " you cast, for being involved and portraying the characters with such depth and gentleness!<br /><br />I recognized the Angry sister; the Runaway sister and\n",
      " the sister in Denial. I recognized the Abusive Husband and why he was there and then the Father, oh oh the Father... all superbly played. I also recognized\n",
      " myself and this movie was an eye-opener, a relief, a chance to face my OWN truth and finally doing something about it. I truly hope A Thousand Acres has had\n",
      " the same effect on some others out there.<br /><br />Since I didn't understand why the cover said the film was about sisters fighting over land -they weren't\n",
      " fighting each other at all- I watched it a second time. Then I was able to see that if one hadn't lived a similar story, one would easily miss the\n",
      " overwhelming undercurrent of dread and fear and the deep bond between the sisters that runs through it all. That is exactly the reason why people in general\n",
      " often overlook the truth about their neighbors for instance.<br /><br />But yet another reason why this movie is so perfect!<br /><br />I don't give a rat's\n",
      " ass (pardon my French) about to what extend the King Lear story is followed. All I know is that I can honestly say: this movie has changed my life.<br /><br\n",
      " />Keep up the good work guys, you CAN and DO make a difference.<br /><br />\n",
      "3. pred=POSITIVE | label=POSITIVE \n",
      " This is the latest entry in the long series of films with the French agent, O.S.S. 117 (the French answer to James Bond). The series was launched in the early\n",
      " 1950's, and spawned at least eight films (none of which was ever released in the U.S.). 'O.S.S.117:Cairo,Nest Of Spies' is a breezy little comedy that should\n",
      " not...repeat NOT, be taken too seriously. Our protagonist finds himself in the middle of a spy chase in Egypt (with Morroco doing stand in for Egypt) to find\n",
      " out about a long lost friend. What follows is the standard James Bond/Inspector Cloussou kind of antics. Although our man is something of an overt\n",
      " xenophobe,sexist,homophobe, it's treated as pure farce (as I said, don't take it too seriously). Although there is a bit of rough language & cartoon violence,\n",
      " it's basically okay for older kids (ages 12 & up). As previously stated in the subject line, just sit back,pass the popcorn & just enjoy.\n"
     ]
    }
   ],
   "source": [
    "# Show the performance of the model on some test set examples\n",
    "\n",
    "# select the first negative and the first positive test set examples\n",
    "\n",
    "\n",
    "predictions = trainer.predict(tokenized_ds[\"test\"].select(dataset_indices))\n",
    "# Print the 10 first test set samples with their predictions.\n",
    "# split text into lines of 80 characters\n",
    "def split_text(text, n=160):\n",
    "    lines = []\n",
    "    while len(text) > n:\n",
    "        line = text[:n]\n",
    "        space_index = line.rfind(\" \")\n",
    "        if space_index != -1:\n",
    "            line = line[:space_index]\n",
    "        lines.append(line)\n",
    "        text = text[len(line):]\n",
    "    lines.append(text)\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "for index, (pred, label, text) in enumerate(zip(predictions.predictions.argmax(axis=1), predictions.label_ids, tokenized_ds[\"test\"].select(dataset_indices)[\"text\"])):\n",
    "    print(f\"{index}. pred={id2label[pred]} | label={id2label[label]} \\n {split_text(text)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
